{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import model_fine as model_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "\n",
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_classes, use_bias=True, use_bn=False, name='linear_layer', **kwargs):\n",
    "        # Note: use_bias is ignored for the dense layer when use_bn =True. However, it is still used for batch norm\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.use_bn = use_bn\n",
    "        self._name = name\n",
    "        if callable(self.num_classes):\n",
    "            num_classes = -1\n",
    "        else:\n",
    "            num_classes = self.num_classes\n",
    "        self.dense = tf.keras.layers.Dense(num_classes, kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "        use_bias=use_bias and not self.use_bn)\n",
    "        if self.use_bn:\n",
    "            self.bn_relu = BatchNormRelu(relu=False, center=use_bias)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if callable(self.num_classes):\n",
    "            self.dense.units = self.num_classes(input_shape)\n",
    "        super(LinearLayer,self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        assert inputs.shape.ndims == 2, inputs.shape\n",
    "        inputs = self.dense(inputs)\n",
    "        if self.use_bn:\n",
    "            inputs = self.bn_relu(inputs, training=training)\n",
    "        return inputs\n",
    "\n",
    "class ProjectionHead(tf.keras.layers.Layer):\n",
    "    #using nonlinear projectionHead\n",
    "    def __init__(self, **kwargs):\n",
    "        self.linear_layers = []\n",
    "        for j in range(3):\n",
    "            if j != 3 - 1:\n",
    "                #for the middle layers, use bias and relu for the output\n",
    "                self.linear_layers.append(LinearLayer(num_classes=lambda input_shape: int(input_shape[-1]),\n",
    "                use_bias=True, use_bn=True, name='nl_%d' % j))\n",
    "            else:\n",
    "                #for the final layer, neither bias nor relu is used\n",
    "                self.linear_layers.append(LinearLayer(num_classes=128, use_bias=False, use_bn=True, name='nl_%d' %j))\n",
    "        \n",
    "        super(ProjectionHead, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        hiddens_list = [tf.identity(inputs, 'proj_head_input')]\n",
    "        for j in range(3):\n",
    "            hiddens = self.linear_layers[j](hiddens_list[-1], training)\n",
    "            if j!= 3 - 1:\n",
    "                #for the middle layers, use bias and relu for the output.\n",
    "                hiddens = tf.nn.relu(hiddens)\n",
    "            hiddens_list.append(hiddens)\n",
    "\n",
    "        #The element is the input of the finetune head\n",
    "        return hiddens_list[0]\n",
    "\n",
    "class SupervisedHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_classes, name='head_supervised', **kwargs):\n",
    "        super(SupervisedHead, self).__init__(name=name, **kwargs)\n",
    "        self.linear_layer = LinearLayer(num_classes)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        inputs = self.linear_layer(inputs, training)\n",
    "        inputs = tf.identity(inputs, name='logits_sup')\n",
    "        return inputs\n",
    "\n",
    "class Model(tf.keras.models.Model):\n",
    "    #Resnet model with supervised layer\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "        #resnet\n",
    "        self.resnet_model = resnet(resnet_depth=18, cifar_stem=False)\n",
    "        self._projection_head = ProjectionHead()\n",
    "        #self.supervised_head = SupervisedHead(num_classes)\n",
    "    \n",
    "    def __call__(self, inputs, training):\n",
    "        features = inputs\n",
    "        num_transforms = 1\n",
    "        \n",
    "        #split channels and optionally apply extra batched augmentation\n",
    "        features_list = tf.split(features, num_or_size_splits=num_transforms, axis=-1)\n",
    "        features = tf.concat(features_list, 0) #(num_transforms * bsz, h, w, c)\n",
    "\n",
    "        #base network forward pass\n",
    "        hiddens = self.resnet_model(features, training=training)\n",
    "\n",
    "        #add heads\n",
    "        supervised_head_inputs = self._projection_head(hiddens, training)\n",
    "\n",
    "        supervised_head_outputs = self.supervised_head(supervised_head_inputs, training)\n",
    "        return supervised_head_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Conv2D function with fixed padding\n",
    "class Conv2Ds(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides, data_format, **kwargs):\n",
    "        super(Conv2Ds, self).__init__(**kwargs)\n",
    "        if strides > 1:\n",
    "            self.fixed_padding = FixedPadding(kernel_size, data_format=data_format)\n",
    "        else:\n",
    "            self.fixed_padding = None\n",
    "\n",
    "        self.conv2d = tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "            padding=('SAME' if strides==1 else 'VALID'), use_bias=False,\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(), data_format=data_format)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        if self.fixed_padding:\n",
    "            inputs = self.fixed_padding(inputs, training=training)\n",
    "        return self.conv2d(inputs, training=training)\n",
    "\n",
    "#necesario para poder sumar input con shortcut en bottleneckres\n",
    "class FixedPadding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, kernel_size, data_format='channels_last', **kwargs):\n",
    "        super(FixedPadding, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.data_format = data_format\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        kernel_size = self.kernel_size\n",
    "        data_format = self.data_format\n",
    "        pad_total = kernel_size -1\n",
    "        pad_beg = pad_total // 2\n",
    "        pad_end = pad_total - pad_beg\n",
    "        if data_format == 'channels_first':\n",
    "            padded_inputs = tf.pad(inputs, [[0,0], [0,0], [pad_beg, pad_end], [pad_beg, pad_end]])\n",
    "        else:\n",
    "            padded_inputs = tf.pad(inputs, [[0,0], [pad_beg, pad_end], [pad_beg, pad_end], [0,0]])\n",
    "\n",
    "        return padded_inputs\n",
    "\n",
    "#Apply batch normalization with or without relu activation\n",
    "class BatchNormRelu(tf.keras.layers.Layer):\n",
    "    def __init__(self, relu=True, init_zero=False, center=True, scale=True, data_format='channels_last', **kwargs):\n",
    "        super(BatchNormRelu, self).__init__(**kwargs)\n",
    "        self.activation = relu\n",
    "        if init_zero:\n",
    "            gamma_initializer = tf.zeros_initializer()\n",
    "        else:\n",
    "            gamma_initializer = tf.ones_initializer()\n",
    "        if data_format == 'channels_first':\n",
    "            bn_axis = 1\n",
    "        else:\n",
    "            bn_axis = -1\n",
    "\n",
    "        if True:\n",
    "            self.bn = tf.keras.layers.experimental.SyncBatchNormalization(\n",
    "                axis=bn_axis, momentum=0.9, epsilon=1.001e-5, center=center,\n",
    "                scale=scale, gamma_initializer=gamma_initializer)\n",
    "        else:\n",
    "            self.bn = tf.keras.layers.BatchNormalization(\n",
    "                axis=bn_axis, momentum=0.9, epsilon=1.001e-5, center=center,\n",
    "                scale=scale, fused=True, gamma_initializer=gamma_initializer)\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        inputs = self.bn(inputs, training=training)\n",
    "        if self.activation:\n",
    "            inputs = tf.nn.relu(inputs)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides, data_format='channels_last', **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.conv2d_layers = []\n",
    "        self.shortcut_layers = []\n",
    "\n",
    "        self.shortcut_layers.append(Conv2Ds(filters=filters, kernel_size=1, strides=strides, data_format=data_format))\n",
    "        self.shortcut_layers.append(BatchNormRelu(relu=False, data_format=data_format))\n",
    "        self.conv2d_layers.append(Conv2Ds(filters=filters, kernel_size=3, strides=strides, data_format=data_format))\n",
    "        self.conv2d_layers.append(BatchNormRelu(data_format=data_format))\n",
    "        self.conv2d_layers.append(Conv2Ds(filters=filters, kernel_size=3, strides=1, data_format=data_format))\n",
    "        self.conv2d_layers.append(BatchNormRelu(relu=False, init_zero=True, data_format=data_format))\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        shortcut = inputs\n",
    "        for layer in self.shortcut_layers:\n",
    "            # Projection shortcut in first layer to match filters and strides\n",
    "            shortcut = layer(shortcut, training=training)\n",
    "        \n",
    "        for layer in self.conv2d_layers:\n",
    "            inputs = layer(inputs, training=training)\n",
    "\n",
    "        return tf.nn.relu(inputs + shortcut)\n",
    "\n",
    "## stack bottleneck layers depending on resnet architecture 18,34,50,101 etc\n",
    "class BlockGroup(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, block_fn, blocks, strides, data_format='channels_last', **kwargs):\n",
    "        self._name = kwargs.get('name')\n",
    "        super(BlockGroup, self).__init__(**kwargs)\n",
    "        self.layers = []\n",
    "        self.layers.append(block_fn(filters, strides, data_format=data_format))\n",
    "        for _ in range(1, blocks):\n",
    "            self.layers.append(block_fn(filters, 1, data_format=data_format))\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs, training=training)\n",
    "        return tf.identity(inputs, self._name)\n",
    "\n",
    "\n",
    "class IdentityLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, training):\n",
    "        return tf.identity(inputs)\n",
    "\n",
    "\n",
    "class Resnet(tf.keras.layers.Layer):\n",
    "    def __init__(self, block_fn, layers, cifar_stem=False, data_format='channels_last', **kwargs):\n",
    "        super(Resnet, self).__init__(**kwargs)\n",
    "        self.data_format = data_format\n",
    "\n",
    "        trainable = True\n",
    "\n",
    "        self.initial_layers = []\n",
    "        if cifar_stem:\n",
    "            self.initial_layers.append(Conv2Ds(filters=64, kernel_size=3, strides=1,\n",
    "                data_format=data_format, trainable=trainable))\n",
    "            self.initial_layers.append(IdentityLayer(name='initial_conv', trainable=trainable))\n",
    "            self.initial_layers.append(BatchNormRelu(data_format=data_format, trainable=trainable))\n",
    "            self.initial_layers.append(IdentityLayer(name='initial_max_pool', trainable=trainable))\n",
    "        else:\n",
    "            self.initial_layers.append(Conv2Ds(filters=64, kernel_size=7, strides=2,\n",
    "                data_format=data_format, trainable=trainable))\n",
    "            self.initial_layers.append(IdentityLayer(name='initial_conv', trainable=trainable))\n",
    "            self.initial_layers.append(BatchNormRelu(data_format=data_format, trainable=trainable))\n",
    "            self.initial_layers.append(tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, \n",
    "                padding='SAME', data_format=data_format, trainable=trainable))\n",
    "            self.initial_layers.append(IdentityLayer(name='initial_max_pool', trainable=trainable))\n",
    "\n",
    "        self.block_groups = []\n",
    "    \n",
    "        #first block\n",
    "        self.block_groups.append(BlockGroup(filters=64, block_fn=block_fn, blocks=layers[0],\n",
    "            strides=1, name='block_group1', data_format=data_format, trainable=trainable))\n",
    "\n",
    "        #second block\n",
    "        self.block_groups.append(BlockGroup(filters=128, block_fn=block_fn, blocks=layers[1],\n",
    "            strides=2, name='block_group2', data_format=data_format, trainable=trainable))\n",
    "\n",
    "              \n",
    "        #third block\n",
    "        self.block_groups.append(BlockGroup(filters=256, block_fn=block_fn, blocks=layers[2],\n",
    "            strides=2, name='block_group3', data_format=data_format, trainable=trainable))\n",
    "\n",
    "        #fourth block\n",
    "        self.block_groups.append(BlockGroup(filters=512, block_fn=block_fn, blocks=layers[3],\n",
    "            strides=2, name='block_group4', data_format=data_format, trainable=trainable))\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        for layer in self.initial_layers:\n",
    "            inputs = layer(inputs, training=training)\n",
    "        \n",
    "        for layer in self.block_groups:\n",
    "            inputs = layer(inputs, training=training)\n",
    "        \n",
    "        if self.data_format == 'channels_last':\n",
    "            inputs = tf.reduce_mean(inputs, [1, 2])\n",
    "        else:\n",
    "            inputs = tf.reduce_mean(inputs, [2, 3])\n",
    "        \n",
    "        inputs = tf.identity(inputs, 'final_avg_pool')\n",
    "        return inputs\n",
    "\n",
    "def resnet(resnet_depth, cifar_stem=False, data_format='channels_last'):\n",
    "    model_params = {\n",
    "        18: {'block': ResidualBlock, 'layers': [2, 2, 2, 2]}}\n",
    "\n",
    "    if resnet_depth not in model_params:\n",
    "        raise ValueError('Not implemented resnet_depth:', resnet_depth)\n",
    "        \n",
    "    params = model_params[resnet_depth]\n",
    "    return Resnet(params['block'], params['layers'], cifar_stem=cifar_stem,data_format=data_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_manager2 = tf.train.CheckpointManager(tf.train.Checkpoint(model=model), directory='drf/', max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_manager2.checkpoint.restore('pretrain/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "weight_decay_metric = tf.keras.metrics.Mean('train/weight_decay')\n",
    "total_loss_metric = tf.keras.metrics.Mean('train/total_loss')\n",
    "supervised_loss_metric = tf.keras.metrics.Mean('train/supervised_loss')\n",
    "supervised_acc_metric = tf.keras.metrics.Mean('train/supervised_acc')\n",
    "all_metrics.extend([weight_decay_metric, total_loss_metric,\n",
    "    supervised_loss_metric, supervised_acc_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_manager = tf.train.CheckpointManager(\n",
    "      checkpoint,\n",
    "      directory='pretrain/',\n",
    "      max_to_keep=1)\n",
    "latest_ckpt = checkpoint_manager.latest_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pretrain/ckpt-13635'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9b0c216b10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_manager2 = tf.train.CheckpointManager(tf.train.Checkpoint(model=model), directory='pedro/', max_to_keep=1)\n",
    "checkpoint_manager2.checkpoint.restore(latest_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = checkpoint_manager2.checkpoint.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Model at 0x7f9b2c8ead50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIM",
   "language": "python",
   "name": "sim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
